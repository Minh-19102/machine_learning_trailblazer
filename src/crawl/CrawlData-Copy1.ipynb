{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d463051f-7cd3-4a2b-bcab-264f59b96c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy cell này đầu tiên\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e18a8c-38e5-4f5f-aece-892116479d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge file\n",
    "# Không cần nhạy cell này\n",
    "'''\n",
    "from unidecode import unidecode\n",
    "\n",
    "json_files = {\n",
    "    \"VN\": \"artist_VietNam.json\", \n",
    "    \"JP\": \"artist_Japan.json\", \n",
    "    \"EU\": \"artist_EU.json\",\n",
    "    \"USUK\": \"artist_USUK.json\",\n",
    "    \"KR\": \"artist_Korea.json\",\n",
    "    \"Latin\": \"artist_Latin.json\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    return unidecode(name.lower())\n",
    "\n",
    "def filter_similar_names(names):\n",
    "    unique_names = set()\n",
    "    filtered_names = []\n",
    "    for name in names:\n",
    "        normalized_name = normalize_name(name)\n",
    "        if normalized_name not in unique_names:\n",
    "            unique_names.add(normalized_name)\n",
    "            filtered_names.append(name)\n",
    "    return filtered_names\n",
    "\n",
    "# Initialize an empty dictionary to store names and areas\n",
    "name_area_dict = {}\n",
    "\n",
    "# Loop through each JSON file\n",
    "for area, json_file in json_files.items():\n",
    "    with open(f\"./{json_file}\", 'r') as file:\n",
    "        names = json.load(file)\n",
    "        filtered_names = filter_similar_names(names)\n",
    "        for name in filtered_names:\n",
    "            if name in name_area_dict:\n",
    "                name_area_dict[name] += f';{area}'\n",
    "            else:\n",
    "                name_area_dict[name] = area\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(name_area_dict.items()), columns=['Name', 'Area'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.sort_values(by=['Name']).to_csv('artistDataset.csv', index=False)\n",
    "print(\"CSV file created successfully.\")\n",
    "'''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d7f355-3dd9-44e7-8b45-5fb3971a4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sau đó chạy cell này\n",
    "def getSongInfo(name, url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        links = soup.select(\".play-this-track-playlinks .play-this-track-playlink\")\n",
    "        result = {}\n",
    "        for link in links:\n",
    "            result[link.getText().strip()] = link['href']\n",
    "        return result\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "def crawlAllSongOfArtist(artist):\n",
    "    try:\n",
    "        allSongsList = []\n",
    "        artistQuery = artist.replace(\"+\", \"%252B\").replace(\" \", \"+\")\n",
    "        url = f\"https://www.last.fm/music/{artistQuery}/+tracks?date_preset=ALL\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        pages = soup.find_all(attrs={\"class\": \"pagination-page\"})\n",
    "        last_page = int(pages[-1].find('a').get_text().strip()) if len(pages) else 1\n",
    "        for i in range(1, last_page+1):\n",
    "        # For test using: \n",
    "        #for i in range(1, 2):\n",
    "            res = requests.get(f\"{url}&page={i}\")\n",
    "            res.raise_for_status()\n",
    "            pg = res.text\n",
    "            soup = BeautifulSoup(pg, 'html.parser')\n",
    "            songs = soup.select(\"tr.chartlist-row > .chartlist-name > a\")\n",
    "            for song in songs:\n",
    "            # For test using: \n",
    "            #for song in songs[:5]:\n",
    "                songName = song.getText()\n",
    "                songUrl = song['href']\n",
    "                allSongsList.append([songName, getSongInfo(songName, f\"https://www.last.fm{songUrl}\")])\n",
    "                print(f\"artist: {artist} - page: {i}/{last_page} - song: {songName}\")\n",
    "                clear_output(wait=True)\n",
    "        return allSongsList\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860860e4-9c1c-4d72-ae7b-1115f89555b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Coding\\AI_ML\\machine_learning_trailblazer\\env\\lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 77\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mpopularity = None\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03mduration = None\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03mrelease_date = None\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYouTube\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m song_info[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 77\u001b[0m     yt_API_res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://api.socialcounts.org/youtube-video-live-view-count/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msong_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYouTube\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m?v=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     yt_view_count \u001b[38;5;241m=\u001b[39m yt_API_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mest_sub\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mest_sub\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m  yt_API_res \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     yt_like_count \u001b[38;5;241m=\u001b[39m yt_API_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m  yt_API_res \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Coding\\AI_ML\\machine_learning_trailblazer\\env\\lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bắt đầu từ đây \n",
    "1. Đổi lại firstIndex và lastIndex dựa vào danh sách nghệ sĩ được yêu cầu\n",
    "Minh: 0 - 6320\n",
    "Ngọc: 6321 - 12640\n",
    "Quân: 12641 - 18960\n",
    "Giang: 18961 - 25280\n",
    "Long: 25280 - 31605\n",
    "2. Chạy cell này. Giữ internet ổn định và cắm máy đấy rồi đi làm việc khác\n",
    "Lưu ý: * Sau mỗi nghệ sĩ crawl thành công sẽ lưu vào file. \n",
    "Do đó có thể chạy tiếp cell này sau khi stop, tiến trình sẽ tiếp tục.\n",
    "Tuy nhiên nên hạn chế để tránh lỗi\n",
    "\n",
    "* Nếu thấy log bên dưới error, lập tức bấm Interrupt và kiểm tra lại internet\n",
    "'''\n",
    "####\n",
    "firstIndex = 12967\n",
    "lastIndex = 13596\n",
    "####\n",
    "target_file_data = None  # Initialize as a list\n",
    "targetFileName = f'./songData{firstIndex}-{lastIndex}.csv'\n",
    "\n",
    "# File danh sách nghệ sĩ\n",
    "artist_df = pd.read_csv('artistDataset.csv')\n",
    "\n",
    "if os.path.exists(targetFileName):\n",
    "    target_file_data = pd.read_csv(targetFileName)\n",
    "else:\n",
    "    column_names = [\n",
    "        \"title\", \n",
    "        \"artist\", \n",
    "        \"area\", \n",
    "        \"youtube_link\", \n",
    "        \"spotify_link\", \n",
    "        \"release_date\", \n",
    "        \"youtube_view_count\", \n",
    "        \"youtube_like_count\", \n",
    "        \"tag\",\n",
    "        \"duration\",\n",
    "        \"popularity\",\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"loudness\",\n",
    "        \"tempo\",\n",
    "    ]\n",
    "    target_file_data = pd.DataFrame(columns=column_names)\n",
    "    target_file_data.to_csv(targetFileName, index=False)\n",
    "    target_file_data = pd.read_csv(targetFileName)\n",
    "\n",
    "for i in range(firstIndex, lastIndex + 1):\n",
    "    dfs_to_concat = []\n",
    "    artist = artist_df.loc[i, \"Name\"]\n",
    "    if artist not in target_file_data[\"artist\"].unique():\n",
    "        for song_info in crawlAllSongOfArtist(artist):\n",
    "            yt_view_count = None\n",
    "            yt_like_count = None\n",
    "            '''\n",
    "            popularity = None\n",
    "            duration = None\n",
    "            danceability = None\n",
    "            energy = None\n",
    "            speechiness = None\n",
    "            acousticness = None\n",
    "            instrumentalness = None\n",
    "            liveness = None\n",
    "            valence = None\n",
    "            loudness = 0\n",
    "            tempo = None\n",
    "            release_date = None\n",
    "            '''\n",
    "            if \"YouTube\" in song_info[1]:\n",
    "                yt_API_res = requests.get(f'https://api.socialcounts.org/youtube-video-live-view-count/{song_info[1][\"YouTube\"].split(\"?v=\")[-1]}').json()\n",
    "                yt_view_count = yt_API_res[\"est_sub\"] if \"est_sub\" in  yt_API_res else None\n",
    "                yt_like_count = yt_API_res[\"table\"][0][\"count\"] if \"table\" in  yt_API_res else None\n",
    "            '''\n",
    "            if \"Spotify\" in song_info[1]:\n",
    "                spotify_id = song_info[1][\"Spotify\"].split(\"/\")[-1]\n",
    "                spotify_feature_API_res = requests.get(f'https://www.chosic.com/api/tools/audio-features/{spotify_id}')\n",
    "                spotify_info_API_res = requests.get(f'https://www.chosic.com/api/tools/tracks/{spotify_id}')\n",
    "                popularity = spotify_info_API_res[\"popularity\"] if \"popularity\" in spotify_info_API_res else None\n",
    "                release_date = spotify_info_API_res[\"album\"][\"release_date\"] if \"album\" in spotify_info_API_res else None\n",
    "                duration = spotify_info_API_res[\"duration_ms\"] if \"duration_ms\" in spotify_info_API_res else None\n",
    "                danceability = spotify_info_API_res[\"danceability\"] if \"danceability\" in spotify_info_API_res else None\n",
    "                energy = spotify_info_API_res[\"energy\"] if \"energy\" in spotify_info_API_res else None\n",
    "                speechiness = spotify_info_API_res[\"speechiness\"] if \"speechiness\" in spotify_info_API_res else None\n",
    "                acousticness = spotify_info_API_res[\"acousticness\"] if \"acousticness\" in spotify_info_API_res else None\n",
    "                instrumentalness = spotify_info_API_res[\"instrumentalness\"] if \"instrumentalness\" in spotify_info_API_res else None\n",
    "                liveness = spotify_info_API_res[\"liveness\"] if \"liveness\" in spotify_info_API_res else None\n",
    "                valence = spotify_info_API_res[\"valence\"] if \"valence\" in spotify_info_API_res else None\n",
    "                loudness = spotify_info_API_res[\"loudness\"] if \"loudness\" in spotify_info_API_res else None\n",
    "                tempo = spotify_info_API_res[\"tempo\"] if \"tempo\" in spotify_info_API_res else None\n",
    "            '''\n",
    "            df_to_append = pd.DataFrame({\n",
    "                \"title\": [song_info[0]],\n",
    "                \"artist\": [artist],\n",
    "                \"area\": [artist_df.loc[i, \"Area\"]],\n",
    "                \"youtube_link\": [song_info[1][\"YouTube\"] if \"YouTube\" in song_info[1] else None],\n",
    "                \"spotify_link\": [song_info[1][\"Spotify\"] if \"Spotify\" in song_info[1] else None],\n",
    "                \"youtube_view_count\": [yt_view_count],\n",
    "                \"youtube_like_count\": [yt_like_count],\n",
    "                #\"duration\": [duration],\n",
    "                #\"danceability\": [danceability],\n",
    "                #\"energy\": [energy],\n",
    "                #\"speechiness\": [speechiness],\n",
    "                #\"acousticness\": [acousticness],\n",
    "                #\"instrumentalness\": [instrumentalness],\n",
    "                #\"liveness\": [liveness],\n",
    "                #\"valence\": [valence],\n",
    "                #\"loudness\": [loudness],\n",
    "                #\"tempo\": [tempo],\n",
    "                \"tag\": None\n",
    "            }) \n",
    "            dfs_to_concat.append(df_to_append)\n",
    "        target_file_data = pd.concat([target_file_data] + dfs_to_concat, ignore_index=True)\n",
    "        target_file_data.to_csv(targetFileName, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cf3b9-668a-4514-9011-c3d6f66f47b8",
   "metadata": {},
   "source": [
    "https://api.socialcounts.org/youtube-video-live-view-count/8xg3vE8Ie_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f50cce-d9b4-4b93-928d-46d8880b2e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb07b4c-d686-4aaf-ac54-2dd2a9915db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7479f0-9dda-49d8-bcc2-fd89245c93d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1cdcb-774b-4ef2-a325-922bcfa25a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
