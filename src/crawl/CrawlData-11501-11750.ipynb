{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d463051f-7cd3-4a2b-bcab-264f59b96c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chạy cell này đầu tiên\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e18a8c-38e5-4f5f-aece-892116479d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge file\n",
    "# Không cần nhạy cell này\n",
    "'''\n",
    "from unidecode import unidecode\n",
    "\n",
    "json_files = {\n",
    "    \"VN\": \"artist_VietNam.json\", \n",
    "    \"JP\": \"artist_Japan.json\", \n",
    "    \"EU\": \"artist_EU.json\",\n",
    "    \"USUK\": \"artist_USUK.json\",\n",
    "    \"KR\": \"artist_Korea.json\",\n",
    "    \"Latin\": \"artist_Latin.json\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    return unidecode(name.lower())\n",
    "\n",
    "def filter_similar_names(names):\n",
    "    unique_names = set()\n",
    "    filtered_names = []\n",
    "    for name in names:\n",
    "        normalized_name = normalize_name(name)\n",
    "        if normalized_name not in unique_names:\n",
    "            unique_names.add(normalized_name)\n",
    "            filtered_names.append(name)\n",
    "    return filtered_names\n",
    "\n",
    "# Initialize an empty dictionary to store names and areas\n",
    "name_area_dict = {}\n",
    "\n",
    "# Loop through each JSON file\n",
    "for area, json_file in json_files.items():\n",
    "    with open(f\"./{json_file}\", 'r') as file:\n",
    "        names = json.load(file)\n",
    "        filtered_names = filter_similar_names(names)\n",
    "        for name in filtered_names:\n",
    "            if name in name_area_dict:\n",
    "                name_area_dict[name] += f';{area}'\n",
    "            else:\n",
    "                name_area_dict[name] = area\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(name_area_dict.items()), columns=['Name', 'Area'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.sort_values(by=['Name']).to_csv('artistDataset.csv', index=False)\n",
    "print(\"CSV file created successfully.\")\n",
    "'''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d7f355-3dd9-44e7-8b45-5fb3971a4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sau đó chạy cell này\n",
    "def getSongInfo(name, url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        links = soup.select(\".play-this-track-playlinks .play-this-track-playlink\")\n",
    "        result = {}\n",
    "        for link in links:\n",
    "            result[link.getText().strip()] = link['href']\n",
    "        return result\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "def crawlAllSongOfArtist(artist):\n",
    "    try:\n",
    "        allSongsList = []\n",
    "        artistQuery = artist.replace(\"+\", \"%252B\").replace(\" \", \"+\")\n",
    "        url = f\"https://www.last.fm/music/{artistQuery}/+tracks?date_preset=ALL\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        page_content = response.text\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        pages = soup.find_all(attrs={\"class\": \"pagination-page\"})\n",
    "        last_page = int(pages[-1].find('a').get_text().strip()) if len(pages) else 1\n",
    "        for i in range(1, last_page+1):\n",
    "        # For test using: \n",
    "        #for i in range(1, 2):\n",
    "            res = requests.get(f\"{url}&page={i}\")\n",
    "            res.raise_for_status()\n",
    "            pg = res.text\n",
    "            soup = BeautifulSoup(pg, 'html.parser')\n",
    "            songs = soup.select(\"tr.chartlist-row > .chartlist-name > a\")\n",
    "            for song in songs:\n",
    "            # For test using: \n",
    "            #for song in songs[:5]:\n",
    "                songName = song.getText()\n",
    "                songUrl = song['href']\n",
    "                allSongsList.append([songName, getSongInfo(songName, f\"https://www.last.fm{songUrl}\")])\n",
    "                print(f\"artist: {artist} - page: {i}/{last_page} - song: {songName}\")\n",
    "                clear_output(wait=True)\n",
    "        return allSongsList\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860860e4-9c1c-4d72-ae7b-1115f89555b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist: La Chavez Special Band - page: 1/1 - song: Autobiografía\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Bắt đầu từ đây \n",
    "1. Đổi lại firstIndex và lastIndex dựa vào danh sách nghệ sĩ được yêu cầu\n",
    "Minh: 0 - 6320\n",
    "Ngọc: 6321 - 12640\n",
    "Quân: 12641 - 18960\n",
    "Giang: 18961 - 25280\n",
    "Long: 25280 - 31605\n",
    "2. Chạy cell này. Giữ internet ổn định và cắm máy đấy rồi đi làm việc khác\n",
    "Lưu ý: * Sau mỗi nghệ sĩ crawl thành công sẽ lưu vào file. \n",
    "Do đó có thể chạy tiếp cell này sau khi stop, tiến trình sẽ tiếp tục.\n",
    "Tuy nhiên nên hạn chế để tránh lỗi\n",
    "\n",
    "* Nếu thấy log bên dưới error, lập tức bấm Interrupt và kiểm tra lại internet\n",
    "'''\n",
    "####\n",
    "firstIndex = 11501\n",
    "lastIndex = 11750\n",
    "####\n",
    "target_file_data = None  # Initialize as a list\n",
    "targetFileName = f'./songData{firstIndex}-{lastIndex}.csv'\n",
    "\n",
    "# File danh sách nghệ sĩ\n",
    "artist_df = pd.read_csv('artistDataset.csv')\n",
    "\n",
    "if os.path.exists(targetFileName):\n",
    "    target_file_data = pd.read_csv(targetFileName)\n",
    "else:\n",
    "    column_names = [\n",
    "        \"title\", \n",
    "        \"artist\", \n",
    "        \"area\", \n",
    "        \"youtube_link\", \n",
    "        \"spotify_link\", \n",
    "        \"release_date\", \n",
    "        \"youtube_view_count\", \n",
    "        \"youtube_like_count\", \n",
    "        \"tag\",\n",
    "        \"duration\",\n",
    "        \"popularity\",\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"loudness\",\n",
    "        \"tempo\",\n",
    "    ]\n",
    "    target_file_data = pd.DataFrame(columns=column_names)\n",
    "    target_file_data.to_csv(targetFileName, index=False)\n",
    "    target_file_data = pd.read_csv(targetFileName)\n",
    "\n",
    "for i in range(firstIndex, lastIndex + 1):\n",
    "    dfs_to_concat = []\n",
    "    artist = artist_df.loc[i, \"Name\"]\n",
    "    if artist not in target_file_data[\"artist\"].unique():\n",
    "        for song_info in crawlAllSongOfArtist(artist):\n",
    "            yt_view_count = None\n",
    "            yt_like_count = None\n",
    "            '''\n",
    "            popularity = None\n",
    "            duration = None\n",
    "            danceability = None\n",
    "            energy = None\n",
    "            speechiness = None\n",
    "            acousticness = None\n",
    "            instrumentalness = None\n",
    "            liveness = None\n",
    "            valence = None\n",
    "            loudness = 0\n",
    "            tempo = None\n",
    "            release_date = None\n",
    "            '''\n",
    "            if \"YouTube\" in song_info[1]:\n",
    "                yt_API_res = requests.get(f'https://api.socialcounts.org/youtube-video-live-view-count/{song_info[1][\"YouTube\"].split(\"?v=\")[-1]}').json()\n",
    "                yt_view_count = yt_API_res[\"est_sub\"] if \"est_sub\" in  yt_API_res else None\n",
    "                yt_like_count = yt_API_res[\"table\"][0][\"count\"] if \"table\" in  yt_API_res else None\n",
    "            '''\n",
    "            if \"Spotify\" in song_info[1]:\n",
    "                spotify_id = song_info[1][\"Spotify\"].split(\"/\")[-1]\n",
    "                spotify_feature_API_res = requests.get(f'https://www.chosic.com/api/tools/audio-features/{spotify_id}')\n",
    "                spotify_info_API_res = requests.get(f'https://www.chosic.com/api/tools/tracks/{spotify_id}')\n",
    "                popularity = spotify_info_API_res[\"popularity\"] if \"popularity\" in spotify_info_API_res else None\n",
    "                release_date = spotify_info_API_res[\"album\"][\"release_date\"] if \"album\" in spotify_info_API_res else None\n",
    "                duration = spotify_info_API_res[\"duration_ms\"] if \"duration_ms\" in spotify_info_API_res else None\n",
    "                danceability = spotify_info_API_res[\"danceability\"] if \"danceability\" in spotify_info_API_res else None\n",
    "                energy = spotify_info_API_res[\"energy\"] if \"energy\" in spotify_info_API_res else None\n",
    "                speechiness = spotify_info_API_res[\"speechiness\"] if \"speechiness\" in spotify_info_API_res else None\n",
    "                acousticness = spotify_info_API_res[\"acousticness\"] if \"acousticness\" in spotify_info_API_res else None\n",
    "                instrumentalness = spotify_info_API_res[\"instrumentalness\"] if \"instrumentalness\" in spotify_info_API_res else None\n",
    "                liveness = spotify_info_API_res[\"liveness\"] if \"liveness\" in spotify_info_API_res else None\n",
    "                valence = spotify_info_API_res[\"valence\"] if \"valence\" in spotify_info_API_res else None\n",
    "                loudness = spotify_info_API_res[\"loudness\"] if \"loudness\" in spotify_info_API_res else None\n",
    "                tempo = spotify_info_API_res[\"tempo\"] if \"tempo\" in spotify_info_API_res else None\n",
    "            '''\n",
    "            df_to_append = pd.DataFrame({\n",
    "                \"title\": [song_info[0]],\n",
    "                \"artist\": [artist],\n",
    "                \"area\": [artist_df.loc[i, \"Area\"]],\n",
    "                \"youtube_link\": [song_info[1][\"YouTube\"] if \"YouTube\" in song_info[1] else None],\n",
    "                \"spotify_link\": [song_info[1][\"Spotify\"] if \"Spotify\" in song_info[1] else None],\n",
    "                \"youtube_view_count\": [yt_view_count],\n",
    "                \"youtube_like_count\": [yt_like_count],\n",
    "                #\"duration\": [duration],\n",
    "                #\"danceability\": [danceability],\n",
    "                #\"energy\": [energy],\n",
    "                #\"speechiness\": [speechiness],\n",
    "                #\"acousticness\": [acousticness],\n",
    "                #\"instrumentalness\": [instrumentalness],\n",
    "                #\"liveness\": [liveness],\n",
    "                #\"valence\": [valence],\n",
    "                #\"loudness\": [loudness],\n",
    "                #\"tempo\": [tempo],\n",
    "                \"tag\": None\n",
    "            }) \n",
    "            dfs_to_concat.append(df_to_append)\n",
    "        target_file_data = pd.concat([target_file_data] + dfs_to_concat, ignore_index=True)\n",
    "        target_file_data.to_csv(targetFileName, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cf3b9-668a-4514-9011-c3d6f66f47b8",
   "metadata": {},
   "source": [
    "https://api.socialcounts.org/youtube-video-live-view-count/8xg3vE8Ie_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3b9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
